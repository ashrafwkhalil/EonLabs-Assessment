# -*- coding: utf-8 -*-
"""Eon Labs Data Collection Assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ashrafwkhalil/EonLabs-Assessment/blob/main/Eon%20Labs%20Data%20Collection%20Assessment.ipynb
"""


"""## Add Necessary Imports"""
import sys
import subprocess

subprocess.check_call([sys.executable, '-m', 'pip', 'install', 
'pytrends'])
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 
'pandas'])

import pandas as pd
from pytrends.request import TrendReq




"""## Connect to Google"""

pytrend= TrendReq()

"""## Build Payload and Make Initial Hourly Request
I use the get_historical_interest API method to get hourly interest data related to the 'Bitcoin' keyword. I will make no other API calls for the rest of this notebook, being that this data is sufficient to derive all other desired data. I had to set the 'sleep' parameter to 0.2 to ensure that some time passes between successive API calls so that I don't get rate limited by Google. Also, to ensure this lengthy API call isn't done multiple times by accident, I check to see if df has already been initialized.
"""

kw_list = ['bitcoin']
try:
  df 
except:
  pytrend.build_payload(kw_list)
  print("fetching data...")
  df = pytrend.get_historical_interest(kw_list, year_start = 2015, month_start = 1, day_start = 1, hour_start = 0, year_end = 2022, month_end = 10, day_end = 1, hour_end = 11, sleep = 0.3)
  print("done")
"""## Hourly Interest
Basically just re-naming and dropping isPartial Column, also resetting index so date can be used for groupby
"""

hourly_interest = df[['bitcoin']]
hourly_interest = hourly_interest.reset_index()
hourly_interest.rename(columns = {'date':'Hour', 'bitcoin':'Interest'}, inplace = True)
hourly_interest

"""## Daily Interest
Just grouping hourly data by day using a custom grouper that groups based on a frequency; in this case, day is the frequency. This returns the daily average interest.
"""

daily_interest = hourly_interest.groupby(pd.Grouper(key = 'Hour', freq = 'D')).mean()
daily_interest = daily_interest.reset_index()
daily_interest.rename(columns = {'Hour':'Day'}, inplace = True)
daily_interest

"""## Weekly Interest
Just grouping hourly data by day using a custom grouper that groups based on a frequency; in this case, day is the frequency. This returns the daily average interest.
"""

weekly_interest = daily_interest.groupby(pd.Grouper(key = 'Day', freq = 'W')).mean()
weekly_interest = weekly_interest.reset_index()
weekly_interest.rename(columns = {'Day':'Week Start (Sunday)'}, inplace = True)
weekly_interest

"""## Create and Write to CSV
I will create 4 csv files, because I am unsure what the assessment requires. I will make one where all the tables are appended to one another, and one for each table.
"""

hourly_interest.to_csv('interest_data_hourly.csv', header = True, index = False)
hourly_interest.to_csv('interest_data.csv', header = True, index = False)
daily_interest.to_csv('interest_data_daily.csv', header = True, index = False)
daily_interest.to_csv('interest_data.csv', header = True, mode = 'a', index = False)
weekly_interest.to_csv('interest_data_weekly.csv', header = True, index = False)
weekly_interest.to_csv('interest_data.csv', header = True, mode = 'a', index = False)

display = pd.read_csv('interest_data.csv')
display

display = pd.read_csv('interest_data_hourly.csv')
display

display = pd.read_csv('interest_data_daily.csv')
display

display = pd.read_csv('interest_data_weekly.csv')
display
